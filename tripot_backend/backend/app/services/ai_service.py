import openai  # ì˜¤íƒ€ ìˆ˜ì •
import asyncio
import json
import os
import base64
import tempfile

from app.core.config import settings
# ìˆœí™˜ ì°¸ì¡°(Circular Dependency)ë¥¼ í”¼í•˜ê¸° ìœ„í•´, ì´ íŒŒì¼ì—ì„œëŠ” ë‹¤ë¥¸ ì„œë¹„ìŠ¤ íŒŒì¼ì„ ì§ì ‘ importí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)

# --- 1. Core AI Utilities (ê¸°ì¡´ê³¼ ìœ ì‚¬) ---

async def get_embedding(text: str) -> list[float]:
    """í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„ë² ë”© ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    response = await asyncio.to_thread(
        client.embeddings.create, input=text, model="text-embedding-3-small"
    )
    return response.data[0].embedding

async def get_transcript_from_audio(audio_file_path: str) -> str:
    """ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ STT(Speech-to-Text) ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with open(audio_file_path, "rb") as audio_file:
        transcript_response = await asyncio.to_thread(
            client.audio.transcriptions.create, model="whisper-1", file=audio_file, language="ko"
        )
    return transcript_response.text

async def get_ai_chat_completion(prompt: str, model: str = "gpt-4o", max_tokens: int = 150, temperature: float = 0.7) -> str:
    """ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ AI ì±—ë´‡ì˜ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ê·œì¹™ê³¼ í˜ë¥´ì†Œë‚˜ë¥¼ ì™„ë²½í•˜ê²Œ ë”°ë¥´ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt}
    ]
    chat_response = await asyncio.to_thread(
        client.chat.completions.create,
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature
    )
    return chat_response.choices[0].message.content

# --- 2. Main Real-time Conversation Logic (í•µì‹¬ ë¡œì§ ì´ë™) ---

def _load_talk_prompt():
    """prompts/talk_prompts.json íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ì–´ì˜¤ëŠ” í—¬í¼ í•¨ìˆ˜"""
    # ğŸ”§ íŒŒì¼ëª… ìˆ˜ì •: talk_prompts.json (s ì¶”ê°€)
    possible_paths = [
        '/backend/prompts/talk_prompts.json',
        os.path.join(os.path.dirname(__file__), '..', '..', 'prompts', 'talk_prompts.json'),
        './prompts/talk_prompts.json'
    ]
    
    for path in possible_paths:
        try:
            print(f"ğŸ” AI ì„œë¹„ìŠ¤ì—ì„œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì‹œë„: {path}")
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)['main_chat_prompt']
                print(f"âœ… AI ì„œë¹„ìŠ¤ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ: {path}")
                return data
        except Exception as e:
            print(f"âŒ AI ì„œë¹„ìŠ¤ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ({path}): {e}")
            continue
    
    print("âŒ AI ì„œë¹„ìŠ¤ì—ì„œ ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨, None ë°˜í™˜")
    return None

PROMPTS_CONFIG = _load_talk_prompt()

async def process_user_audio(user_id: str, audio_base64: str):
    """
    ì‚¬ìš©ìì˜ ìŒì„± ë°ì´í„°ë¥¼ ë°›ì•„ ì²˜ë¦¬í•˜ê³ , AIì˜ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    (ê¸°ì¡´ main.pyì˜ process_audio_and_get_response ë¡œì§ì„ ì´ê³³ìœ¼ë¡œ ì´ë™)
    """
    try:
        print(f"ğŸµ AI ì„œë¹„ìŠ¤ ì‹œì‘: {user_id}")
        print(f"ğŸµ ë°›ì€ ì˜¤ë””ì˜¤ í¬ê¸°: {len(audio_base64)} bytes")
        
        # ğŸ”§ vector_db ì„í¬íŠ¸ë¥¼ try-catchë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            from . import vector_db_service as vector_db
            print("âœ… vector_db ì„í¬íŠ¸ ì„±ê³µ")
        except ImportError:
            print("âŒ vector_db ì„í¬íŠ¸ ì‹¤íŒ¨, ê¸°ì–µ ê²€ìƒ‰ ì—†ì´ ì§„í–‰")
            vector_db = None

        # ğŸ”§ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
        audio_data = base64.b64decode(audio_base64)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_audio:
            temp_audio.write(audio_data)
            temp_audio_path = temp_audio.name
        
        try:
            # ğŸ”§ ìŒì„± ì¸ì‹ ì‹œë„
            try:
                user_message = await get_transcript_from_audio(temp_audio_path)
                print(f"âœ… ìŒì„± ì¸ì‹ ê²°ê³¼: {user_message}")
            except Exception as e:
                print(f"âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {str(e)}")
                # ìŒì„± ì¸ì‹ ì‹¤íŒ¨ ì‹œ ì„ì‹œ ë©”ì‹œì§€ ì‚¬ìš©
                user_message = "ì•ˆë…•í•˜ì„¸ìš”"
            
            if not user_message.strip() or "ì‹œì²­í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤" in user_message:
                return None, "ìŒ, ì˜ ì•Œì•„ë“£ì§€ ëª»í–ˆì–´ìš”. í˜¹ì‹œ ë‹¤ì‹œ í•œë²ˆ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
            
            # ğŸ”§ ë²¡í„° DB ê¸°ì–µ ê²€ìƒ‰ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
            relevant_memories = ""
            if vector_db:
                try:
                    relevant_memories = await vector_db.search_memories(user_id, user_message)
                    print(f"âœ… ê¸°ì–µ ê²€ìƒ‰ ì™„ë£Œ")
                except Exception as e:
                    print(f"âŒ ê¸°ì–µ ê²€ìƒ‰ ì‹¤íŒ¨ (ë¬´ì‹œ): {str(e)}")
                    relevant_memories = ""
            
            # ğŸ”§ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            if not PROMPTS_CONFIG:
                print("âŒ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì—†ìŒ, ê¸°ë³¸ ì‘ë‹µ ì‚¬ìš©")
                return user_message, "ëŒ€í™” í”„ë¡¬í”„íŠ¸ ì„¤ì • íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì‘ë‹µì„ ë“œë¦½ë‹ˆë‹¤."

            system_message = "\n".join(PROMPTS_CONFIG['system_message_base'])
            examples_text = "\n\n".join([f"ìƒí™©: {ex['situation']}\nì‚¬ìš©ì ì…ë ¥: {ex['user_input']}\nAI ì‘ë‹µ: {ex['ai_response']}" for ex in PROMPTS_CONFIG['examples']])
            
            final_prompt = f"""# í˜ë¥´ì†Œë‚˜\n{system_message}\n# í•µì‹¬ ëŒ€í™” ê·œì¹™\n{"\n".join(PROMPTS_CONFIG['core_conversation_rules'])}\n# ì‘ë‹µ ê°€ì´ë“œë¼ì¸\n{"\n".join(PROMPTS_CONFIG['guidelines_and_reactions'])}\n# ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­\n{"\n".join(PROMPTS_CONFIG['strict_prohibitions'])}\n# ì„±ê³µì ì¸ ëŒ€í™” ì˜ˆì‹œ\n{examples_text}\n---\nì´ì œ ì‹¤ì œ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n--- ê³¼ê±° ëŒ€í™” í•µì‹¬ ê¸°ì–µ ---\n{relevant_memories if relevant_memories else "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}\n--------------------\ní˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"\nAI ë‹µë³€:"""
            
            # ğŸ”§ AI ì‘ë‹µ ìƒì„±
            try:
                ai_response = await get_ai_chat_completion(final_prompt)
                print(f"âœ… AI ì‘ë‹µ ìƒì„± ì™„ë£Œ: {ai_response[:50]}...")
            except Exception as e:
                print(f"âŒ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                ai_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ ë¬¸ì œê°€ ìˆì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
            
            return user_message, ai_response
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_audio_path)
            except:
                pass
                
    except Exception as e:
        print(f"âŒ AI ì„œë¹„ìŠ¤ ì „ì²´ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None, "ì£„ì†¡í•©ë‹ˆë‹¤. ìŒì„± ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."

# --- 3. Report Generation Logic (for background scripts) ---

def _get_report_prompt():
    """prompts/report_prompt.json íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” í—¬í¼ í•¨ìˆ˜"""
    prompt_file_path = os.path.join(os.path.dirname(__file__), '..', '..', 'prompts', 'report_prompt.json')
    try:
        with open(prompt_file_path, 'r', encoding='utf-8') as f:
            return json.load(f).get("report_analysis_prompt")
    except Exception as e:
        print(f"âŒ report_prompt.json íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def generate_summary_report(conversation_text: str) -> dict | None:
    """ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ì˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ë™ê¸° ë°©ì‹)."""
    
    report_prompt_template = _get_report_prompt()
    if not conversation_text or not report_prompt_template:
        return None

    persona = report_prompt_template.get('persona', 'ë‹¹ì‹ ì€ ì „ë¬¸ ëŒ€í™” ë¶„ì„ AIì…ë‹ˆë‹¤.')
    instructions = "\n".join(report_prompt_template.get('instructions', []))
    output_format_example = json.dumps(report_prompt_template.get('OUTPUT_FORMAT', {}), ensure_ascii=False, indent=2)

    system_prompt = f"{persona}\n\n### ì§€ì‹œì‚¬í•­\n{instructions}\n\n### ì¶œë ¥ í˜•ì‹\nëª¨ë“  ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì¸ì‚¬ë§ ë“± JSON ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n{output_format_example}"
    user_prompt = f"### ë¶„ì„í•  ëŒ€í™” ì „ë¬¸\n---\n{conversation_text}\n---"

    try:
        completion = client.chat.completions.create(
            model="gpt-4o", 
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return json.loads(completion.choices[0].message.content)
    except Exception as e:
        print(f"AI ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None